{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python368jvsc74a57bd0505a982ef2482993979e4a6536c13153c78bc753f0d5e050478edcb5029d8819",
      "display_name": "Python 3.9.5  ('venv': venv)"
    },
    "accelerator": "TPU",
    "metadata": {
      "interpreter": {
        "hash": "fe8117271c42ec231b15aa672143395ad6e30915725bb2f474e61de5db4d8ccd"
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98kMTYHT1XuX",
        "colab_type": "text"
      },
      "source": [
        "Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xds4ZUHYAI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from pyvi import ViTokenizer, ViPosTagger\n",
        "from utils.transformer import tokenizer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import gensim\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.layers import *\n",
        "from keras.layers import *\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
        "from keras.optimizer_v2.adam import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from configs import ROOT_DIR\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'utils'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-4-ecd9bbb0b13a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# from pyvi import ViTokenizer, ViPosTagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe91-HU6YKXC",
        "colab_type": "text"
      },
      "source": [
        "Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG_EI7STYM8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# datanewscontent=pd.read_excel(r'path_to_newscontent.xlsx','Sheet1')\n",
        "dataset_file_path = ROOT_DIR + '\\datamining\\data\\shopee\\data.xlsx'\n",
        "# dataset_file_path = ROOT_DIR + '\\datamining\\data\\shopee\\comments.xlsx'\n",
        "datacomment=pd.read_excel(dataset_file_path,'Sheet1')\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PueKMfKNYJM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datacmt=[]\n",
        "labelcmt=datacomment['label']\n",
        "new_labelcmt = []\n",
        "i=0\n",
        "a=0\n",
        "b=0\n",
        "c=0\n",
        "for d in datacomment['comment']:\n",
        "    if labelcmt[i] == 1 :\n",
        "        new_labelcmt.append(0)\n",
        "        e=tokenizer(str(d))\n",
        "        datacmt.append(e)\n",
        "        a+=1\n",
        "    if labelcmt[i] == 3 :\n",
        "        new_labelcmt.append(1)\n",
        "        e=tokenizer(str(d))\n",
        "        datacmt.append(e)\n",
        "        b+=1\n",
        "    if labelcmt[i] == 5 :\n",
        "        new_labelcmt.append(2)\n",
        "        e=tokenizer(str(d))\n",
        "        datacmt.append(e)\n",
        "        c+=1\n",
        "    i+=1\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "\n",
        "print(len(datacmt))\n",
        "print(len(labelcmt))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4114\n2935\n1999\n9048\n15030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iwLE6kTj8fQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# datanews=[]\n",
        "# for d in datanewscontent['all_lower']:\n",
        "#     e=ViTokenizer.tokenize(str(d))\n",
        "#     datanews.append(e)\n",
        "# labelnews=datanewscontent['label']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWMDeVkV0LwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def truncatedvectors(data,n_components=300):\n",
        "  svd_ngram = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "  svd_ngram.fit(data)\n",
        "  return svd_ngram.transform(data)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go30TYZAYWZy",
        "colab_type": "text"
      },
      "source": [
        "Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRlyTezBYOkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec,KeyedVectors \n",
        "import os\n",
        "word2vec_model_path =\"path_to_wikivimodel.bin\"\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_model_path,binary=True, unicode_errors='ignore')\n",
        "vocab = model.wv.vocab\n",
        "wv = model.wv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyYQobJsNupy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word2vec_data(X):\n",
        "    word2vec_data = []\n",
        "    for x in X:\n",
        "        sentence = []\n",
        "        for word in x.split(\" \"):\n",
        "            if word in vocab:\n",
        "              sentence=sentence+wv[word].ravel().tolist()\n",
        "        word2vec_data.append(sentence)\n",
        "\n",
        "    return word2vec_data\n",
        "def change_to_word2vec(data):\n",
        "  data2vec=get_word2vec_data(data)\n",
        "  lengthOfdata=[len(data2vec[i]) for i,n in enumerate(data2vec)]\n",
        "  for i,n in enumerate(data):\n",
        "    if(len(data2vec[i])<max(lengthOfdata)):\n",
        "      for j in range(1,(max(lengthOfdata)-len(data2vec[i]))+1):\n",
        "        data2vec[i].append(0)\n",
        "  return truncatedvectors(np.array(data2vec))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HcprRIKNyKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_data_w2v_cmt=change_to_word2vec(datacmt)\n",
        "X_data_w2v_news=change_to_word2vec(datanews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1Ik7LvtNqqf",
        "colab_type": "text"
      },
      "source": [
        "TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKVZl4PfNsGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tfidf(data):\n",
        "  tfidf_vect_ngram = TfidfVectorizer(analyzer='word', max_features=30000, ngram_range=(1, 2))\n",
        "  tfidf_vect_ngram.fit(data)\n",
        "  X_data_tfidf_ngram =  tfidf_vect_ngram.transform(data)\n",
        "  return truncatedvectors(X_data_tfidf_ngram)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3_r6g-H0R5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_data_tfidf_cmt=tfidf(datacmt)\n",
        "# X_data_tfidf_news=tfidf(datanews)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYNrb3eoOGFP",
        "colab_type": "text"
      },
      "source": [
        "Bag of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hdw-F11OHz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bow(data):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(dataall)\n",
        "  datacmtbow = tokenizer.texts_to_sequences(data)\n",
        "  datacmtbow= pad_sequences(datacmtbow, maxlen=300)\n",
        "  return datacmtbow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKsJt9F80WT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_data_bow_cmt=bow(datacmt)\n",
        "X_data_bow_news=bow(datanews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQQKE-giTDbd",
        "colab_type": "text"
      },
      "source": [
        "LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c80buTPvN_AK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Change: [X_data_tfidf_cmt,  X_data_w2v_cmt, X_data_bow_cmt],labelcmt , [X_data_tfidf_news,X_data_bow_news,X_data_w2v_news], labelnews\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_data_tfidf_cmt, new_labelcmt, test_size=0.2, random_state=42)\n",
        "t = []\n",
        "for y in y_val:\n",
        "    if not y in t :\n",
        "        t.append(y)\n",
        "print(t)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 0, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj72P0RAMZAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#learning_rate: [1e-5,5e-5,1e-4,5e-4,1e-3,5e-3]\n",
        "opt = Adam(lr=0.001)\n",
        "def create_lstm_model():\n",
        "    input_layer = Input(shape=(300,))\n",
        "    layer = Reshape((10, 30,))(input_layer)\n",
        "    layer = LSTM(128, activation='relu',dropout=0.5, recurrent_dropout=0.5)(layer)\n",
        "    layer = Dense(64, activation='relu')(layer)\n",
        "    layer = Dense(32, activation='relu')(layer)\n",
        "    output_layer = Dense(3, activation='softmax')(layer)\n",
        "    classifier = Model(input_layer, output_layer)\n",
        "    classifier.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return classifier"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\Users\\huanc\\Desktop\\Ky2Nam4\\Duan\\CommentRate\\venv\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:357: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFaWc5hFMfYX",
        "colab_type": "code",
        "colab": {},
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "y_train_lstm_encode = to_categorical(y_train)\n",
        "print(y_train_lstm_encode[1000])\n",
        "y_val_lstm_encode=to_categorical(y_val)\n",
        "classifier = create_lstm_model()\n",
        "classifier.fit(X_train, y_train_lstm_encode, validation_data=(X_val, y_val_lstm_encode), epochs=200, batch_size=32)\n",
        "#change different epoch and batch_size"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ss: 0.6460 - val_accuracy: 0.7055\n",
            "Epoch 64/200\n",
            "227/227 [==============================] - 6s 26ms/step - loss: 0.6693 - accuracy: 0.6879 - val_loss: 0.6465 - val_accuracy: 0.7066\n",
            "Epoch 65/200\n",
            "227/227 [==============================] - 6s 25ms/step - loss: 0.6660 - accuracy: 0.6877 - val_loss: 0.6405 - val_accuracy: 0.6989\n",
            "Epoch 66/200\n",
            "227/227 [==============================] - 5s 24ms/step - loss: 0.6693 - accuracy: 0.6890 - val_loss: 0.6466 - val_accuracy: 0.7028\n",
            "Epoch 67/200\n",
            "227/227 [==============================] - 6s 25ms/step - loss: 0.6614 - accuracy: 0.7016 - val_loss: 0.6468 - val_accuracy: 0.6967\n",
            "Epoch 68/200\n",
            "227/227 [==============================] - 6s 24ms/step - loss: 0.6687 - accuracy: 0.6989 - val_loss: 0.6395 - val_accuracy: 0.6989\n",
            "Epoch 69/200\n",
            "227/227 [==============================] - 5s 24ms/step - loss: 0.6758 - accuracy: 0.6921 - val_loss: 0.6626 - val_accuracy: 0.6956\n",
            "Epoch 70/200\n",
            "227/227 [==============================] - 6s 25ms/step - loss: 0.6563 - accuracy: 0.7015 - val_loss: 0.6405 - val_accuracy: 0.7028\n",
            "Epoch 71/200\n",
            "227/227 [==============================] - 6s 25ms/step - loss: 0.6495 - accuracy: 0.6981 - val_loss: 0.6417 - val_accuracy: 0.7011\n",
            "Epoch 72/200\n",
            "227/227 [==============================] - 6s 25ms/step - loss: 0.6530 - accuracy: 0.6973 - val_loss: 0.6308 - val_accuracy: 0.7050\n",
            "Epoch 73/200\n",
            "227/227 [==============================] - 6s 25ms/step - loss: 0.6669 - accuracy: 0.6951 - val_loss: 0.6377 - val_accuracy: 0.6994\n",
            "Epoch 74/200\n",
            "227/227 [==============================] - 5s 22ms/step - loss: 0.6511 - accuracy: 0.7015 - val_loss: 0.6428 - val_accuracy: 0.7055\n",
            "Epoch 75/200\n",
            "227/227 [==============================] - 6s 27ms/step - loss: 0.6616 - accuracy: 0.6993 - val_loss: 0.6453 - val_accuracy: 0.7055\n",
            "Epoch 76/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.6362 - accuracy: 0.7117 - val_loss: 0.6442 - val_accuracy: 0.7033\n",
            "Epoch 77/200\n",
            "227/227 [==============================] - 6s 27ms/step - loss: 0.6461 - accuracy: 0.7040 - val_loss: 0.6476 - val_accuracy: 0.6928\n",
            "Epoch 78/200\n",
            "227/227 [==============================] - 5s 22ms/step - loss: 0.6512 - accuracy: 0.7007 - val_loss: 0.6470 - val_accuracy: 0.6961\n",
            "Epoch 79/200\n",
            "227/227 [==============================] - 6s 25ms/step - loss: 0.6554 - accuracy: 0.7002 - val_loss: 0.6381 - val_accuracy: 0.7028\n",
            "Epoch 80/200\n",
            "227/227 [==============================] - 6s 25ms/step - loss: 0.6474 - accuracy: 0.7031 - val_loss: 0.6356 - val_accuracy: 0.7044\n",
            "Epoch 81/200\n",
            "227/227 [==============================] - 5s 22ms/step - loss: 0.6512 - accuracy: 0.6985 - val_loss: 0.6326 - val_accuracy: 0.7094\n",
            "Epoch 82/200\n",
            "227/227 [==============================] - 5s 22ms/step - loss: 0.6490 - accuracy: 0.7037 - val_loss: 0.6508 - val_accuracy: 0.7055\n",
            "Epoch 83/200\n",
            "227/227 [==============================] - 5s 20ms/step - loss: 0.6482 - accuracy: 0.7119 - val_loss: 0.6434 - val_accuracy: 0.7110\n",
            "Epoch 84/200\n",
            "227/227 [==============================] - 5s 24ms/step - loss: 0.6529 - accuracy: 0.7053 - val_loss: 0.6533 - val_accuracy: 0.6967\n",
            "Epoch 85/200\n",
            "227/227 [==============================] - 8s 34ms/step - loss: 0.6560 - accuracy: 0.7016 - val_loss: 0.6725 - val_accuracy: 0.7061\n",
            "Epoch 86/200\n",
            "227/227 [==============================] - 7s 31ms/step - loss: 0.6580 - accuracy: 0.6939 - val_loss: 0.6402 - val_accuracy: 0.7077\n",
            "Epoch 87/200\n",
            "227/227 [==============================] - 5s 24ms/step - loss: 0.6351 - accuracy: 0.7097 - val_loss: 0.6360 - val_accuracy: 0.7127\n",
            "Epoch 88/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.6467 - accuracy: 0.7031 - val_loss: 0.6353 - val_accuracy: 0.7138\n",
            "Epoch 89/200\n",
            "227/227 [==============================] - 4s 19ms/step - loss: 0.6490 - accuracy: 0.7076 - val_loss: 0.6348 - val_accuracy: 0.7133\n",
            "Epoch 90/200\n",
            "227/227 [==============================] - 5s 24ms/step - loss: 0.6443 - accuracy: 0.6989 - val_loss: 0.6340 - val_accuracy: 0.7000\n",
            "Epoch 91/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.6424 - accuracy: 0.7007 - val_loss: 0.6364 - val_accuracy: 0.7055\n",
            "Epoch 92/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.6415 - accuracy: 0.7070 - val_loss: 0.6612 - val_accuracy: 0.7006\n",
            "Epoch 93/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.6524 - accuracy: 0.6860 - val_loss: 0.6611 - val_accuracy: 0.7050\n",
            "Epoch 94/200\n",
            "227/227 [==============================] - 7s 30ms/step - loss: 0.6347 - accuracy: 0.7041 - val_loss: 0.6376 - val_accuracy: 0.7110\n",
            "Epoch 95/200\n",
            "227/227 [==============================] - 5s 22ms/step - loss: 0.6385 - accuracy: 0.7042 - val_loss: 0.6422 - val_accuracy: 0.7006\n",
            "Epoch 96/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.6502 - accuracy: 0.7070 - val_loss: 0.6287 - val_accuracy: 0.7077\n",
            "Epoch 97/200\n",
            "227/227 [==============================] - 5s 20ms/step - loss: 0.6342 - accuracy: 0.7108 - val_loss: 0.6476 - val_accuracy: 0.6961\n",
            "Epoch 98/200\n",
            "227/227 [==============================] - 5s 22ms/step - loss: 0.6246 - accuracy: 0.7178 - val_loss: 0.6340 - val_accuracy: 0.7050\n",
            "Epoch 99/200\n",
            "227/227 [==============================] - 7s 31ms/step - loss: 0.6331 - accuracy: 0.7119 - val_loss: 0.6295 - val_accuracy: 0.7088\n",
            "Epoch 100/200\n",
            "227/227 [==============================] - 6s 28ms/step - loss: 0.6347 - accuracy: 0.7102 - val_loss: 0.6428 - val_accuracy: 0.7050\n",
            "Epoch 101/200\n",
            "227/227 [==============================] - 6s 27ms/step - loss: 0.6486 - accuracy: 0.7055 - val_loss: 0.6326 - val_accuracy: 0.7099\n",
            "Epoch 102/200\n",
            "227/227 [==============================] - 6s 27ms/step - loss: 0.6473 - accuracy: 0.7062 - val_loss: 0.6380 - val_accuracy: 0.7122\n",
            "Epoch 103/200\n",
            "227/227 [==============================] - 6s 27ms/step - loss: 0.6254 - accuracy: 0.7053 - val_loss: 0.6297 - val_accuracy: 0.7122\n",
            "Epoch 104/200\n",
            "227/227 [==============================] - 6s 27ms/step - loss: 0.6258 - accuracy: 0.7222 - val_loss: 0.6292 - val_accuracy: 0.7061\n",
            "Epoch 105/200\n",
            "227/227 [==============================] - 6s 26ms/step - loss: 0.6332 - accuracy: 0.7124 - val_loss: 0.6296 - val_accuracy: 0.7138\n",
            "Epoch 106/200\n",
            "227/227 [==============================] - 6s 26ms/step - loss: 0.6258 - accuracy: 0.7090 - val_loss: 0.6341 - val_accuracy: 0.7055\n",
            "Epoch 107/200\n",
            "227/227 [==============================] - 6s 27ms/step - loss: 0.6148 - accuracy: 0.7266 - val_loss: 0.6312 - val_accuracy: 0.7072\n",
            "Epoch 108/200\n",
            "227/227 [==============================] - 6s 26ms/step - loss: 0.6372 - accuracy: 0.7080 - val_loss: 0.6281 - val_accuracy: 0.7138\n",
            "Epoch 109/200\n",
            "227/227 [==============================] - 6s 27ms/step - loss: 0.6246 - accuracy: 0.7090 - val_loss: 0.6466 - val_accuracy: 0.7055\n",
            "Epoch 110/200\n",
            "227/227 [==============================] - 6s 28ms/step - loss: 0.6331 - accuracy: 0.7116 - val_loss: 0.6321 - val_accuracy: 0.7166\n",
            "Epoch 111/200\n",
            "227/227 [==============================] - 6s 26ms/step - loss: 0.6391 - accuracy: 0.7140 - val_loss: 0.6348 - val_accuracy: 0.7033\n",
            "Epoch 112/200\n",
            "227/227 [==============================] - 7s 30ms/step - loss: 0.6308 - accuracy: 0.7085 - val_loss: 0.6323 - val_accuracy: 0.7055\n",
            "Epoch 113/200\n",
            "227/227 [==============================] - 6s 27ms/step - loss: 0.6100 - accuracy: 0.7171 - val_loss: 0.6473 - val_accuracy: 0.7072\n",
            "Epoch 114/200\n",
            "227/227 [==============================] - 7s 32ms/step - loss: 0.6361 - accuracy: 0.7081 - val_loss: 0.6302 - val_accuracy: 0.7138\n",
            "Epoch 115/200\n",
            "227/227 [==============================] - 6s 27ms/step - loss: 0.6409 - accuracy: 0.7083 - val_loss: 0.6641 - val_accuracy: 0.7055\n",
            "Epoch 116/200\n",
            "227/227 [==============================] - 6s 27ms/step - loss: 0.6383 - accuracy: 0.7074 - val_loss: 0.6311 - val_accuracy: 0.7072\n",
            "Epoch 117/200\n",
            "227/227 [==============================] - 6s 26ms/step - loss: 0.6275 - accuracy: 0.7115 - val_loss: 0.6270 - val_accuracy: 0.7127\n",
            "Epoch 118/200\n",
            "227/227 [==============================] - 6s 27ms/step - loss: 0.6225 - accuracy: 0.7125 - val_loss: 0.6420 - val_accuracy: 0.7171\n",
            "Epoch 119/200\n",
            "227/227 [==============================] - 6s 27ms/step - loss: 0.6363 - accuracy: 0.7049 - val_loss: 0.6436 - val_accuracy: 0.7022\n",
            "Epoch 120/200\n",
            "227/227 [==============================] - 6s 25ms/step - loss: 0.6263 - accuracy: 0.7054 - val_loss: 0.6357 - val_accuracy: 0.7099\n",
            "Epoch 121/200\n",
            "227/227 [==============================] - 6s 27ms/step - loss: 0.6104 - accuracy: 0.7260 - val_loss: 0.6265 - val_accuracy: 0.7149\n",
            "Epoch 122/200\n",
            "227/227 [==============================] - 6s 27ms/step - loss: 0.6202 - accuracy: 0.7142 - val_loss: 0.6350 - val_accuracy: 0.7066\n",
            "Epoch 123/200\n",
            "227/227 [==============================] - 6s 27ms/step - loss: 0.6329 - accuracy: 0.7049 - val_loss: 0.6413 - val_accuracy: 0.7039\n",
            "Epoch 124/200\n",
            "227/227 [==============================] - 6s 26ms/step - loss: 0.6321 - accuracy: 0.7093 - val_loss: 0.6429 - val_accuracy: 0.7099\n",
            "Epoch 125/200\n",
            "227/227 [==============================] - 6s 27ms/step - loss: 0.6080 - accuracy: 0.7240 - val_loss: 0.6363 - val_accuracy: 0.7193\n",
            "Epoch 126/200\n",
            "227/227 [==============================] - 6s 28ms/step - loss: 0.6169 - accuracy: 0.7130 - val_loss: 0.6305 - val_accuracy: 0.7127\n",
            "Epoch 127/200\n",
            "227/227 [==============================] - 6s 27ms/step - loss: 0.6245 - accuracy: 0.7211 - val_loss: 0.6258 - val_accuracy: 0.7127\n",
            "Epoch 128/200\n",
            "227/227 [==============================] - 6s 24ms/step - loss: 0.6248 - accuracy: 0.7092 - val_loss: 0.6349 - val_accuracy: 0.7138\n",
            "Epoch 129/200\n",
            "227/227 [==============================] - 4s 20ms/step - loss: 0.6357 - accuracy: 0.7059 - val_loss: 0.6345 - val_accuracy: 0.7127\n",
            "Epoch 130/200\n",
            "227/227 [==============================] - 5s 24ms/step - loss: 0.6143 - accuracy: 0.7263 - val_loss: 0.6274 - val_accuracy: 0.7066\n",
            "Epoch 131/200\n",
            "227/227 [==============================] - 6s 29ms/step - loss: 0.6104 - accuracy: 0.7210 - val_loss: 0.6331 - val_accuracy: 0.7055\n",
            "Epoch 132/200\n",
            "227/227 [==============================] - 5s 22ms/step - loss: 0.6126 - accuracy: 0.7150 - val_loss: 0.6376 - val_accuracy: 0.7133\n",
            "Epoch 133/200\n",
            "227/227 [==============================] - 5s 24ms/step - loss: 0.6075 - accuracy: 0.7301 - val_loss: 0.6437 - val_accuracy: 0.7133\n",
            "Epoch 134/200\n",
            "227/227 [==============================] - 5s 24ms/step - loss: 0.6217 - accuracy: 0.7214 - val_loss: 0.6226 - val_accuracy: 0.7166\n",
            "Epoch 135/200\n",
            "227/227 [==============================] - 5s 22ms/step - loss: 0.6374 - accuracy: 0.7041 - val_loss: 0.6316 - val_accuracy: 0.7133\n",
            "Epoch 136/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.6074 - accuracy: 0.7205 - val_loss: 0.6284 - val_accuracy: 0.7160\n",
            "Epoch 137/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.6034 - accuracy: 0.7133 - val_loss: 0.6384 - val_accuracy: 0.7077\n",
            "Epoch 138/200\n",
            "227/227 [==============================] - 5s 23ms/step - loss: 0.6037 - accuracy: 0.7277 - val_loss: 0.6348 - val_accuracy: 0.7011\n",
            "Epoch 139/200\n",
            "227/227 [==============================] - 5s 22ms/step - loss: 0.6248 - accuracy: 0.7213 - val_loss: 0.6251 - val_accuracy: 0.7088\n",
            "Epoch 140/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.6005 - accuracy: 0.7304 - val_loss: 0.6532 - val_accuracy: 0.6994\n",
            "Epoch 141/200\n",
            "227/227 [==============================] - 5s 23ms/step - loss: 0.6027 - accuracy: 0.7325 - val_loss: 0.6341 - val_accuracy: 0.7138\n",
            "Epoch 142/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.6119 - accuracy: 0.7242 - val_loss: 0.6332 - val_accuracy: 0.7133\n",
            "Epoch 143/200\n",
            "227/227 [==============================] - 6s 24ms/step - loss: 0.6187 - accuracy: 0.7189 - val_loss: 0.6327 - val_accuracy: 0.7127\n",
            "Epoch 144/200\n",
            "227/227 [==============================] - 7s 29ms/step - loss: 0.5911 - accuracy: 0.7242 - val_loss: 0.6351 - val_accuracy: 0.7138\n",
            "Epoch 145/200\n",
            "227/227 [==============================] - 5s 23ms/step - loss: 0.6019 - accuracy: 0.7217 - val_loss: 0.6335 - val_accuracy: 0.7127\n",
            "Epoch 146/200\n",
            "227/227 [==============================] - 5s 23ms/step - loss: 0.6041 - accuracy: 0.7219 - val_loss: 0.6361 - val_accuracy: 0.7122\n",
            "Epoch 147/200\n",
            "227/227 [==============================] - 5s 20ms/step - loss: 0.6125 - accuracy: 0.7200 - val_loss: 0.6568 - val_accuracy: 0.6972\n",
            "Epoch 148/200\n",
            "227/227 [==============================] - 5s 20ms/step - loss: 0.6199 - accuracy: 0.7116 - val_loss: 0.6310 - val_accuracy: 0.7127\n",
            "Epoch 149/200\n",
            "227/227 [==============================] - 5s 20ms/step - loss: 0.5972 - accuracy: 0.7204 - val_loss: 0.6370 - val_accuracy: 0.7138\n",
            "Epoch 150/200\n",
            "227/227 [==============================] - 5s 22ms/step - loss: 0.5858 - accuracy: 0.7401 - val_loss: 0.6349 - val_accuracy: 0.7138\n",
            "Epoch 151/200\n",
            "227/227 [==============================] - 5s 22ms/step - loss: 0.5882 - accuracy: 0.7303 - val_loss: 0.6220 - val_accuracy: 0.7149\n",
            "Epoch 152/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.5932 - accuracy: 0.7347 - val_loss: 0.6367 - val_accuracy: 0.7171\n",
            "Epoch 153/200\n",
            "227/227 [==============================] - 5s 24ms/step - loss: 0.6109 - accuracy: 0.7192 - val_loss: 0.6320 - val_accuracy: 0.7144\n",
            "Epoch 154/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.6103 - accuracy: 0.7225 - val_loss: 0.6410 - val_accuracy: 0.7105\n",
            "Epoch 155/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.5968 - accuracy: 0.7224 - val_loss: 0.6490 - val_accuracy: 0.7099\n",
            "Epoch 156/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.5990 - accuracy: 0.7349 - val_loss: 0.6282 - val_accuracy: 0.7088\n",
            "Epoch 157/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.6093 - accuracy: 0.7163 - val_loss: 0.6324 - val_accuracy: 0.7171\n",
            "Epoch 158/200\n",
            "227/227 [==============================] - 5s 20ms/step - loss: 0.5930 - accuracy: 0.7256 - val_loss: 0.6276 - val_accuracy: 0.7127\n",
            "Epoch 159/200\n",
            "227/227 [==============================] - 5s 20ms/step - loss: 0.6065 - accuracy: 0.7254 - val_loss: 0.6194 - val_accuracy: 0.7127\n",
            "Epoch 160/200\n",
            "227/227 [==============================] - 5s 20ms/step - loss: 0.5950 - accuracy: 0.7312 - val_loss: 0.6288 - val_accuracy: 0.7116\n",
            "Epoch 161/200\n",
            "227/227 [==============================] - 5s 22ms/step - loss: 0.6099 - accuracy: 0.7119 - val_loss: 0.6308 - val_accuracy: 0.7171\n",
            "Epoch 162/200\n",
            "227/227 [==============================] - 5s 22ms/step - loss: 0.5883 - accuracy: 0.7339 - val_loss: 0.6384 - val_accuracy: 0.7210\n",
            "Epoch 163/200\n",
            "227/227 [==============================] - 5s 22ms/step - loss: 0.5995 - accuracy: 0.7247 - val_loss: 0.6233 - val_accuracy: 0.7088\n",
            "Epoch 164/200\n",
            "227/227 [==============================] - 5s 22ms/step - loss: 0.6053 - accuracy: 0.7227 - val_loss: 0.6207 - val_accuracy: 0.7177\n",
            "Epoch 165/200\n",
            "227/227 [==============================] - 5s 20ms/step - loss: 0.6042 - accuracy: 0.7269 - val_loss: 0.6246 - val_accuracy: 0.7199\n",
            "Epoch 166/200\n",
            "227/227 [==============================] - 6s 26ms/step - loss: 0.6092 - accuracy: 0.7191 - val_loss: 0.6257 - val_accuracy: 0.7177\n",
            "Epoch 167/200\n",
            "227/227 [==============================] - 5s 24ms/step - loss: 0.6147 - accuracy: 0.7129 - val_loss: 0.6299 - val_accuracy: 0.7127\n",
            "Epoch 168/200\n",
            "227/227 [==============================] - 5s 24ms/step - loss: 0.6048 - accuracy: 0.7222 - val_loss: 0.6275 - val_accuracy: 0.7138\n",
            "Epoch 169/200\n",
            "227/227 [==============================] - 5s 20ms/step - loss: 0.6022 - accuracy: 0.7250 - val_loss: 0.6335 - val_accuracy: 0.7122\n",
            "Epoch 170/200\n",
            "227/227 [==============================] - 6s 28ms/step - loss: 0.5730 - accuracy: 0.7423 - val_loss: 0.6253 - val_accuracy: 0.7204\n",
            "Epoch 171/200\n",
            "227/227 [==============================] - 5s 24ms/step - loss: 0.5987 - accuracy: 0.7237 - val_loss: 0.6322 - val_accuracy: 0.7083\n",
            "Epoch 172/200\n",
            "227/227 [==============================] - 6s 24ms/step - loss: 0.5844 - accuracy: 0.7226 - val_loss: 0.6356 - val_accuracy: 0.7221\n",
            "Epoch 173/200\n",
            "227/227 [==============================] - 5s 23ms/step - loss: 0.5942 - accuracy: 0.7285 - val_loss: 0.6426 - val_accuracy: 0.7083\n",
            "Epoch 174/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.5814 - accuracy: 0.7323 - val_loss: 0.6404 - val_accuracy: 0.7094\n",
            "Epoch 175/200\n",
            "227/227 [==============================] - 5s 22ms/step - loss: 0.5887 - accuracy: 0.7372 - val_loss: 0.6282 - val_accuracy: 0.7083\n",
            "Epoch 176/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.6081 - accuracy: 0.7204 - val_loss: 0.6366 - val_accuracy: 0.7099\n",
            "Epoch 177/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.5873 - accuracy: 0.7261 - val_loss: 0.6237 - val_accuracy: 0.7127\n",
            "Epoch 178/200\n",
            "227/227 [==============================] - 4s 20ms/step - loss: 0.6001 - accuracy: 0.7309 - val_loss: 0.6508 - val_accuracy: 0.7094\n",
            "Epoch 179/200\n",
            "227/227 [==============================] - 4s 19ms/step - loss: 0.6007 - accuracy: 0.7222 - val_loss: 0.6387 - val_accuracy: 0.7110\n",
            "Epoch 180/200\n",
            "227/227 [==============================] - 4s 19ms/step - loss: 0.5907 - accuracy: 0.7316 - val_loss: 0.6277 - val_accuracy: 0.7193\n",
            "Epoch 181/200\n",
            "227/227 [==============================] - 4s 19ms/step - loss: 0.5905 - accuracy: 0.7280 - val_loss: 0.6395 - val_accuracy: 0.7099\n",
            "Epoch 182/200\n",
            "227/227 [==============================] - 4s 19ms/step - loss: 0.5671 - accuracy: 0.7449 - val_loss: 0.6358 - val_accuracy: 0.7066\n",
            "Epoch 183/200\n",
            "227/227 [==============================] - 4s 19ms/step - loss: 0.5899 - accuracy: 0.7370 - val_loss: 0.6352 - val_accuracy: 0.7149\n",
            "Epoch 184/200\n",
            "227/227 [==============================] - 5s 22ms/step - loss: 0.5734 - accuracy: 0.7415 - val_loss: 0.6483 - val_accuracy: 0.7061\n",
            "Epoch 185/200\n",
            "227/227 [==============================] - 6s 24ms/step - loss: 0.6132 - accuracy: 0.7266 - val_loss: 0.6264 - val_accuracy: 0.7204\n",
            "Epoch 186/200\n",
            "227/227 [==============================] - 5s 24ms/step - loss: 0.6036 - accuracy: 0.7306 - val_loss: 0.6322 - val_accuracy: 0.7094\n",
            "Epoch 187/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.5827 - accuracy: 0.7355 - val_loss: 0.6515 - val_accuracy: 0.7033\n",
            "Epoch 188/200\n",
            "227/227 [==============================] - 5s 20ms/step - loss: 0.5764 - accuracy: 0.7296 - val_loss: 0.6309 - val_accuracy: 0.7083\n",
            "Epoch 189/200\n",
            "227/227 [==============================] - 5s 20ms/step - loss: 0.5701 - accuracy: 0.7410 - val_loss: 0.6237 - val_accuracy: 0.7166\n",
            "Epoch 190/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.5644 - accuracy: 0.7422 - val_loss: 0.6310 - val_accuracy: 0.7182\n",
            "Epoch 191/200\n",
            "227/227 [==============================] - 5s 20ms/step - loss: 0.5740 - accuracy: 0.7482 - val_loss: 0.6335 - val_accuracy: 0.7204\n",
            "Epoch 192/200\n",
            "227/227 [==============================] - 6s 25ms/step - loss: 0.5762 - accuracy: 0.7358 - val_loss: 0.6264 - val_accuracy: 0.7088\n",
            "Epoch 193/200\n",
            "227/227 [==============================] - 5s 21ms/step - loss: 0.5823 - accuracy: 0.7358 - val_loss: 0.6435 - val_accuracy: 0.7033\n",
            "Epoch 194/200\n",
            "227/227 [==============================] - 6s 27ms/step - loss: 0.5816 - accuracy: 0.7337 - val_loss: 0.6286 - val_accuracy: 0.7182\n",
            "Epoch 195/200\n",
            "227/227 [==============================] - 6s 28ms/step - loss: 0.5773 - accuracy: 0.7263 - val_loss: 0.6370 - val_accuracy: 0.7105\n",
            "Epoch 196/200\n",
            "227/227 [==============================] - 6s 25ms/step - loss: 0.5932 - accuracy: 0.7356 - val_loss: 0.6282 - val_accuracy: 0.7155\n",
            "Epoch 197/200\n",
            "227/227 [==============================] - 5s 23ms/step - loss: 0.5690 - accuracy: 0.7386 - val_loss: 0.6352 - val_accuracy: 0.7077\n",
            "Epoch 198/200\n",
            "227/227 [==============================] - 6s 24ms/step - loss: 0.5694 - accuracy: 0.7449 - val_loss: 0.6491 - val_accuracy: 0.7011\n",
            "Epoch 199/200\n",
            "227/227 [==============================] - 6s 25ms/step - loss: 0.5723 - accuracy: 0.7411 - val_loss: 0.6299 - val_accuracy: 0.7144\n",
            "Epoch 200/200\n",
            "227/227 [==============================] - 5s 22ms/step - loss: 0.5711 - accuracy: 0.7440 - val_loss: 0.6443 - val_accuracy: 0.7094\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x2685ca0dc70>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kyz8ERVQPRLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = classifier.predict(X_val, batch_size=32, verbose=1)\n",
        "y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "print(classification_report(y_val, y_pred_bool))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 3s 5ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.75      0.76       828\n",
            "           1       0.66      0.54      0.59       592\n",
            "           2       0.67      0.88      0.76       390\n",
            "\n",
            "    accuracy                           0.71      1810\n",
            "   macro avg       0.70      0.72      0.70      1810\n",
            "weighted avg       0.71      0.71      0.70      1810\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}